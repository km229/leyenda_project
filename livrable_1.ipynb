{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Livrable 1"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import des dépendances"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pathlib\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Paramètres\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "current_dir = pathlib.Path().resolve()\n",
    "DATASET_SIZE = len(list(current_dir.glob('data/*/*')))\n",
    "\n",
    "batch_size = 32\n",
    "img_height = 180\n",
    "img_width = 180\n",
    "num_classes = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41555\n",
      "29088\n",
      "6233\n",
      "6234\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "full_dataset = tf.data.Dataset.list_files(str(current_dir / 'data/*/*'), shuffle=False)\n",
    "full_dataset = full_dataset.shuffle(DATASET_SIZE, reshuffle_each_iteration=False)\n",
    "\n",
    "train_size = int(0.7 * DATASET_SIZE)\n",
    "val_size = int(0.15 * DATASET_SIZE)\n",
    "test_size = int(0.15 * DATASET_SIZE)\n",
    "\n",
    "train_dataset = full_dataset.take(train_size)\n",
    "test_dataset = full_dataset.skip(train_size)\n",
    "val_dataset = test_dataset.skip(val_size)\n",
    "test_dataset = test_dataset.take(test_size)\n",
    "\n",
    "print(DATASET_SIZE)\n",
    "print(tf.data.experimental.cardinality(train_dataset).numpy())\n",
    "print(tf.data.experimental.cardinality(test_dataset).numpy())\n",
    "print(tf.data.experimental.cardinality(val_dataset).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dataset' 'Painting' 'Photo' 'Schematics' 'Sketch' 'Text']\n"
     ]
    }
   ],
   "source": [
    "class_names = np.array(sorted([item.name for item in current_dir.glob('data/*')]))\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_label(file_path):\n",
    "  # Convert the path to a list of path components\n",
    "  parts = tf.strings.split(file_path, os.path.sep)\n",
    "  # The second to last is the class-directory\n",
    "  one_hot = parts[-2] == class_names\n",
    "  # Integer encode the label\n",
    "  return tf.argmax(one_hot)\n",
    "\n",
    "def decode_img(img):\n",
    "  # Convert the compressed string to a 3D uint8 tensor\n",
    "  img = tf.io.decode_jpeg(img, channels=3)\n",
    "  # Resize the image to the desired size\n",
    "  return tf.image.resize(img, [img_height, img_width])\n",
    "\n",
    "def process_path(file_path):\n",
    "  label = get_label(file_path)\n",
    "  # Load the raw data from the file as a string\n",
    "  img = tf.io.read_file(file_path)\n",
    "  img = decode_img(img)\n",
    "  return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TakeDataset shapes: (), types: tf.string>\n",
      "<SkipDataset shapes: (), types: tf.string>\n",
      "<TakeDataset shapes: (), types: tf.string>\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.data import AUTOTUNE\n",
    "\n",
    "train_ds = train_dataset.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "val_ds = val_dataset.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "test_ds = test_dataset.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "print(train_dataset)\n",
    "print(val_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape:  (180, 180, 3)\n",
      "Label:  2\n"
     ]
    }
   ],
   "source": [
    "for image, label in train_ds.take(1):\n",
    "  print(\"Image shape: \", image.numpy().shape)\n",
    "  print(\"Label: \", label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def configure_for_performance(ds):\n",
    "  ds = ds.cache()\n",
    "  ds = ds.shuffle(buffer_size=10000)\n",
    "  ds = ds.batch(batch_size)\n",
    "  ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "  return ds\n",
    "\n",
    "train_ds = configure_for_performance(train_ds)\n",
    "val_ds = configure_for_performance(val_ds)\n",
    "test_ds = configure_for_performance(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardisation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Rescaling(1./255),\n",
    "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Dropout(0.1),\n",
    "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Dropout(0.1),\n",
    "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Dropout(0.1),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(num_classes)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [],
   "source": [
    "#Utilisation des rappels de point de contrôle\n",
    "\n",
    "# Include the epoch in the file name (uses `str.format`)\n",
    "checkpoint_path = \"training/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "909/909 [==============================] - ETA: 0s - loss: 0.5943 - accuracy: 0.7493\n",
      "Epoch 00001: saving model to training\\cp-0001.ckpt\n",
      "909/909 [==============================] - 1242s 1s/step - loss: 0.5943 - accuracy: 0.7493 - val_loss: 0.4194 - val_accuracy: 0.8330\n",
      "Epoch 2/3\n",
      "901/909 [============================>.] - ETA: 11s - loss: 0.3801 - accuracy: 0.8511"
     ]
    }
   ],
   "source": [
    "# # Create a callback that saves the model's weights every 5 epochs\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    verbose=1,\n",
    "    save_weights_only=True)\n",
    "\n",
    "#Entraînement du modèle\n",
    "model.fit(\n",
    "  train_ds,\n",
    "  callbacks=[cp_callback],\n",
    "  validation_data=val_ds,\n",
    "  epochs=3,\n",
    "  shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "# latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "# # Load the previously saved weights\n",
    "# model.load_weights(latest)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unique_labels = np.unique(class_names)\n",
    "\n",
    "def get_pred_label(prediction_probabilities):\n",
    "  \"\"\"\n",
    "  Turns an array of prediction probabilities into a label.\n",
    "  \"\"\"\n",
    "\n",
    "  return unique_labels[np.argmax(prediction_probabilities)]\n",
    "\n",
    "def plot_pred(prediction_probabilities, labels, images, n=1):\n",
    "  \"\"\"\n",
    "  View the prediction, ground truth and image for sample n\n",
    "  \"\"\"\n",
    "  pred_prob, true_label, image = prediction_probabilities[n], labels[n], images[n]\n",
    "\n",
    "  # Get the pred label\n",
    "  pred_label = get_pred_label(pred_prob)\n",
    "\n",
    "  # Plot image and remove ticks\n",
    "  plt.imshow(image)\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "\n",
    "  # Change the color of the title depending on if the prediction is right or wrong\n",
    "  if pred_label == true_label:\n",
    "    color=\"green\"\n",
    "  else:\n",
    "    color=\"red\"\n",
    "\n",
    "  # Change plot title to be predicted, probability of prediction and truth\n",
    "  plt.title(\"{} {:2.0f}% {}\".format(pred_label,\n",
    "                                    np.max(pred_prob)*100,\n",
    "                                    true_label),\n",
    "                                    color=color)\n",
    "\n",
    "def unbatchify(data):\n",
    "  \"\"\"\n",
    "  Takes a batch dataset of (image, label) Tensors and returns separate arrays of images and labels\n",
    "  \"\"\"\n",
    "  images = []\n",
    "  labels = []\n",
    "\n",
    "  # Loop through unbatched data\n",
    "  for image, label in data.unbatch().as_numpy_iterator():\n",
    "    images.append(image)\n",
    "    labels.append(unique_labels[np.argmax(label)])\n",
    "\n",
    "  return images,labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions = model.predict(val_ds)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot_pred(prediction_probabilities=predictions,\n",
    "#           labels=test_label,\n",
    "#           images=test_img,\n",
    "#           n=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# history = model.history\n",
    "#\n",
    "# acc = history.history['accuracy']\n",
    "# val_acc = history.history['val_accuracy']\n",
    "#\n",
    "# loss = history.history['loss']\n",
    "# val_loss = history.history['val_loss']\n",
    "#\n",
    "# epochs_range = range(1)\n",
    "#\n",
    "# plt.figure(figsize=(16, 8))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "# plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.title('Training and Validation Accuracy')\n",
    "#\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.plot(epochs_range, loss, label='Training Loss')\n",
    "# plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "# plt.legend(loc='upper right')\n",
    "# plt.title('Training and Validation Loss')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model.summary()\n",
    "# print(test_ds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}